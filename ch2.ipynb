{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import sqrt\n",
      "import numpy as np\n",
      "import numpy.linalg as LA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### L2 Norm\n",
      "\n",
      "Calculated on vectors of real values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def l2_norm(vector):\n",
      "    # User defined L2 Norm\n",
      "    return sqrt(sum(vector ** 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = np.arange(5)\n",
      "\n",
      "print v\n",
      "\n",
      "l2_norm(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1 2 3 4]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "5.477225575051661"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Speed test\n",
      "\n",
      "#%timeit sum(v)    # This is fastest\n",
      "\n",
      "#%timeit v.sum()\n",
      "\n",
      "#%timeit np.sum(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy.linalg as LA\n",
      "\n",
      "# Numpy builtin L2 norm\n",
      "print LA.norm(v)\n",
      "\n",
      "# builtin L1 norm\n",
      "print LA.norm(v, ord=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5.47722557505\n",
        "10\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Speed test\n",
      "\n",
      "#%timeit l2_norm(v)\n",
      "\n",
      "#%timeit LA.norm(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No surprising speed differences here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a matrix\n",
      "\n",
      "m = np.arange(15).reshape((5, 3))\n",
      "\n",
      "print m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0  1  2]\n",
        " [ 3  4  5]\n",
        " [ 6  7  8]\n",
        " [ 9 10 11]\n",
        " [12 13 14]]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# norms on a matrix\n",
      "\n",
      "# The whole matrix\n",
      "print LA.norm(m)\n",
      "\n",
      "# Column vectors\n",
      "print LA.norm(m, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "31.8590646441\n",
        "[ 16.43167673  18.30300522  20.24845673]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Inner product"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v1 = np.arange(5)\n",
      "\n",
      "print v1\n",
      "\n",
      "v2 = np.ones_like(v1)\n",
      "\n",
      "print v2\n",
      "\n",
      "print 'inner product is %s' % v1.dot(v2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1 2 3 4]\n",
        "[1 1 1 1 1]\n",
        "inner product is 10\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Normalized Vectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# User defined\n",
      "\n",
      "def user_normalize(vector):\n",
      "    return vector / LA.norm(vector)\n",
      "\n",
      "print user_normalize(v)\n",
      "\n",
      "# Always expect the norm to be 1\n",
      "print LA.norm(user_normalize(v))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.          0.18257419  0.36514837  0.54772256  0.73029674]\n",
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Scikit learn\n",
      "\n",
      "from sklearn.preprocessing import normalize\n",
      "\n",
      "# Random uniform values\n",
      "m = 5 * np.random.rand(3, 4)\n",
      "m_normed = normalize(m, axis=0)\n",
      "\n",
      "print 'originals: \\n%s' % m\n",
      "\n",
      "print '\\n normalized: \\n%s' % m_normed\n",
      "\n",
      "print '\\n norms of original values: \\n%s' % LA.norm(m, axis=0)\n",
      "\n",
      "print '\\n norms of new values: \\n%s' % LA.norm(m_normed, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "originals: \n",
        "[[ 0.12562848  2.79467079  1.82141317  2.00591245]\n",
        " [ 3.11325424  4.96223793  0.4559767   0.10551689]\n",
        " [ 3.86585228  2.81069128  0.29235518  2.6107545 ]]\n",
        "\n",
        " normalized: \n",
        "[[ 0.02530193  0.44004302  0.95851476  0.60894763]\n",
        " [ 0.62701821  0.78134362  0.23995676  0.03203244]\n",
        " [ 0.77859359  0.44256558  0.15385128  0.79256338]]\n",
        "\n",
        " norms of original values: \n",
        "[ 4.96517354  6.35090353  1.90024531  3.294064  ]\n",
        "\n",
        " norms of new values: \n",
        "[ 1.  1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def project(x, y):\n",
      "    'project the vector x onto y'\n",
      "    return y * x.dot(y) / LA.norm(y)\n",
      "\n",
      "project(np.array([1, 1, 1]), np.array([1, 0, 0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "array([ 1.,  0.,  0.])"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gs_orthonorm(matrix):\n",
      "    '''\n",
      "    Perform Gram-Schmidt Orthonormalization on a set of vectors contained in a matrix\n",
      "    '''\n",
      "\n",
      "    epsilon = 1e-8\n",
      "\n",
      "    vec_list = [[matrix[:, i], False] for i in range(m.shape[1])]\n",
      "\n",
      "    for vector, flag in vec_list:\n",
      "\n",
      "        if all(abs(vector) < epsilon):\n",
      "            raise FloatingPointError('Are the vectors linearly independent?')\n",
      "    \n",
      "        # Normalize the vector\n",
      "        vector = vector / LA.norm(vector)  # PROBLEM- NOT UPDATING IN PLACE\n",
      "    \n",
      "        # Set the flag indicating that it's been normalized\n",
      "        flag = True\n",
      "    \n",
      "        for vector2, flag2 in vec_list:\n",
      "        \n",
      "            # looping over vectors not yet updated\n",
      "            if not flag2:\n",
      "            \n",
      "                # update all other vectors\n",
      "                vector2 = vector2 - project(vector2, vector)\n",
      "            \n",
      "                if all(abs(vector2) < epsilon):\n",
      "                    raise FloatingPointError('Are the vectors linearly independent?')\n",
      "                    \n",
      "    return(np.matrix([x[0] for x in vec_list]).T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_orthonorm(m)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "matrix([[0, 1],\n",
        "        [2, 3],\n",
        "        [4, 5],\n",
        "        [6, 7],\n",
        "        [8, 9]])"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.matrix(vec_list).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "matrix([[0, 1],\n",
        "        [2, 3],\n",
        "        [4, 5],\n",
        "        [6, 7],\n",
        "        [8, 9]])"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abs(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "array([0, 1, 2, 3, 4])"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}